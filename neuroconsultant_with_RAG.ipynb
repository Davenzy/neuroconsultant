{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1gmUrkMjFawt4trtzPc7QiUFunFRhv4_z",
      "authorship_tag": "ABX9TyMUP6cM6ZQYARtCHw2Oy6Sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a656f92f1bdb461d8481eb7ffa8c22f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f48b968244954276b23e1f49789af133",
              "IPY_MODEL_d2fee17b7ee84048a1fc438b928654b4",
              "IPY_MODEL_1f0a60f5156b45329f144c4f469a7b16"
            ],
            "layout": "IPY_MODEL_8d61c388ca134675a86e567bfe8d677f"
          }
        },
        "f48b968244954276b23e1f49789af133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e51ca0f070314070b52d3ac1fa2bf5f2",
            "placeholder": "​",
            "style": "IPY_MODEL_4c52b221263d480a978559cb4dcca2cf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d2fee17b7ee84048a1fc438b928654b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4bfed2b7e7940e4b1e279cf188453e8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1bb5b31db904b2689570050564e2e28",
            "value": 2
          }
        },
        "1f0a60f5156b45329f144c4f469a7b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1efe76d3260545c78d7ed82510cf6f34",
            "placeholder": "​",
            "style": "IPY_MODEL_99a7d69ce2fb4f95a84839911c7501c0",
            "value": " 2/2 [01:09&lt;00:00, 32.36s/it]"
          }
        },
        "8d61c388ca134675a86e567bfe8d677f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51ca0f070314070b52d3ac1fa2bf5f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c52b221263d480a978559cb4dcca2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4bfed2b7e7940e4b1e279cf188453e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1bb5b31db904b2689570050564e2e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1efe76d3260545c78d7ed82510cf6f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a7d69ce2fb4f95a84839911c7501c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davenzy/neuroconsultant/blob/main/neuroconsultant_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предысловие\n",
        "Идеей для создания данного проекта была личная проблема получения консультации. Были взяты документы реальной копании, которые находились в общем доступе на их официальном сайте.\n",
        "\n",
        "Результатом создания данного консультанта является в первую очередь демонстрация моих технических навыков."
      ],
      "metadata": {
        "id": "DZ0YE9qDeKKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install llama-index pyvis Ipython langchain pypdf langchain_community chromadb\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install langchain-huggingface\n",
        "!pip install sentencepiece accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaot6JSUks-_",
        "outputId": "b5370ec5-8087-4276-9cad-8b2fb7201c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nx_h4p9z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-nx_h4p9z\n",
            "  Resolved https://github.com/huggingface/transformers to commit a3d69a8994d673899608a7c17fbf4f953f50474e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (2024.8.30)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.11.23)\n",
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: Ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"huggingface-hub==0.23.2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "MgFg53YP6hJl",
        "outputId": "b1da8c27-efbb-44e0-cb14-aa127eee042d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface-hub==0.23.2\n",
            "  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (2024.8.30)\n",
            "Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.26.2\n",
            "    Uninstalling huggingface-hub-0.26.2:\n",
            "      Successfully uninstalled huggingface-hub-0.26.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.0.0 requires huggingface-hub>=0.25.1, but you have huggingface-hub 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "79a32d9eb29741578b991292b74f969a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gradio==4.44.1\""
      ],
      "metadata": {
        "id": "rxroeqAl5raF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02741202-63e8-4415-8d8e-53309408e21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio==4.44.1 in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.23.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.8.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.9.2)\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic')"
      ],
      "metadata": {
        "id": "AZnLeQiS7YtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "#from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from pyvis.network import Network"
      ],
      "metadata": {
        "id": "2nDqywUhf4vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "CwR_MTeCR9je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface  import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "PiaDEGEfSAXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для работы с API HuggingFace требуется указать токен, который можно получить зарегестрировавшись на их сайте."
      ],
      "metadata": {
        "id": "oZWuHK5zfGTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "HF_TOKEN=\"{your_hugging_face_token}\"\n",
        "# Вставьте ваш токен (здесь указан временный токен)\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJx2Ym24tadP",
        "outputId": "aa363e4b-51c3-4bc4-df82-d4f8a0439696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для выполнения поставленной задачи была исползована рускоязычная бесплатная LLM - saiga_mistral_7b"
      ],
      "metadata": {
        "id": "o_75hJ3RfgdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем параметры квантования для оптимизации LLM\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Задаем имя модели\n",
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "\n",
        "# Создание конфига, соответствующего методу PEFT (в нашем случае LoRA)\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Загружаем базовую модель, ее имя берем из конфига для LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,          # идентификатор модели\n",
        "    quantization_config=quantization_config, # параметры квантования\n",
        "    torch_dtype=torch.float16,               # тип данных\n",
        "    device_map=\"auto\"                        # автоматический выбор типа устройства\n",
        ")\n",
        "\n",
        "# Загружаем LoRA модель\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Переводим модель в режим инференса\n",
        "# Можно не переводить, но явное всегда лучше неявного\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a656f92f1bdb461d8481eb7ffa8c22f8",
            "f48b968244954276b23e1f49789af133",
            "d2fee17b7ee84048a1fc438b928654b4",
            "1f0a60f5156b45329f144c4f469a7b16",
            "8d61c388ca134675a86e567bfe8d677f",
            "e51ca0f070314070b52d3ac1fa2bf5f2",
            "4c52b221263d480a978559cb4dcca2cf",
            "a4bfed2b7e7940e4b1e279cf188453e8",
            "b1bb5b31db904b2689570050564e2e28",
            "1efe76d3260545c78d7ed82510cf6f34",
            "99a7d69ce2fb4f95a84839911c7501c0"
          ]
        },
        "id": "ZoZrxbdttip1",
        "outputId": "bbe19b26-fd38-4d60-d822-fb2254d60e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a656f92f1bdb461d8481eb7ffa8c22f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32002, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обращения к скачанной LLM требуется указать точный prompt"
      ],
      "metadata": {
        "id": "t7pq_MdqfSij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для создания prompt\n",
        "def messages_to_prompt(messages):\n",
        "  prompt = \"\"\n",
        "  for message in messages:\n",
        "    if message.role == 'system':\n",
        "      prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'user':\n",
        "      prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'bot':\n",
        "      prompt += f\"<s>bot\\n\"\n",
        "\n",
        "  if not prompt.startwith(\"<s>system\\n\"):\n",
        "    prompt = \"<s>system\\n</s>\\n\" + prompt\n",
        "\n",
        "  prompt += \"<s>bot\\n\"\n",
        "  return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "  return f\"<s>system\\n</s>\\n<s>user\\n{completion}</s>\\n<s>bot\\n\""
      ],
      "metadata": {
        "id": "DyF6BYILtfZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# инициализация токенизатора\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# инициализация модели и ее насстроек\n",
        "llm = HuggingFaceLLM(\n",
        "    model=model,             # модель\n",
        "    model_name=MODEL_NAME,   # идентификатор модели\n",
        "    tokenizer=tokenizer,     # токенизатор\n",
        "    max_new_tokens=generation_config.max_new_tokens, # параметр необходимо использовать здесь, и не использовать в generate_kwargs, иначе ошибка двойного использования\n",
        "    model_kwargs={\"quantization_config\": quantization_config}, # параметры квантования\n",
        "    generate_kwargs = {   # параметры для инференса\n",
        "      \"bos_token_id\": generation_config.bos_token_id, # токен начала последовательности\n",
        "      \"eos_token_id\": generation_config.eos_token_id, # токен окончания последовательности\n",
        "      \"pad_token_id\": generation_config.pad_token_id, # токен пакетной обработки (указывает, что последовательность ещё не завершена)\n",
        "      \"no_repeat_ngram_size\": generation_config.no_repeat_ngram_size,\n",
        "      \"repetition_penalty\": generation_config.repetition_penalty,\n",
        "      \"temperature\": generation_config.temperature,\n",
        "      \"do_sample\": True,\n",
        "      \"top_k\": 50,\n",
        "      \"top_p\": 0.95\n",
        "    },\n",
        "    messages_to_prompt=messages_to_prompt,     # функция для преобразования сообщений к внутреннему формату\n",
        "    completion_to_prompt=completion_to_prompt, # функции для генерации текста\n",
        "    device_map=\"auto\",                         # автоматически определять устройство\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vndyTplctojn",
        "outputId": "82296ab3-5732-4666-e2d5-37ea8c52d5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.huggingface.base:The model `Open-Orca/Mistral-7B-OpenOrca` and tokenizer `IlyaGusev/saiga_mistral_7b` are different, please ensure that they are compatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# определяем модель для эмбеддинга данных\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        ")\n",
        "\n",
        "# Настройка ServiceContext (глобальная настройка параметров LLM)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "id": "1PnBDrLCtrkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "# загрузим документы с помощью SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(input_dir=\"documents\").load_data()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents=documents)\n",
        "query_engine = index.as_query_engine(include_text=True)"
      ],
      "metadata": {
        "id": "kdAjdUTht6YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка работоспособности:"
      ],
      "metadata": {
        "id": "rJanfTsEgkQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Where to buy milk?\""
      ],
      "metadata": {
        "id": "5t3McpS90xBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_template =f\"\"\"<s>system\n",
        "        Отвечай в соответствии с Источником. Проверь, есть ли в Источнике упоминания о ключевых словах Вопроса.\n",
        "        Если нет, то просто скажи: 'я не знаю'. Не придумывай!</s>\n",
        "        <s>user\n",
        "        Вопрос: {query}\n",
        "        Источник:\n",
        "        </s>\n",
        "        \"\"\"\n",
        "\n",
        "response = query_engine.query(message_template)\n",
        "\n",
        "print()\n",
        "print('Ответ:')\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "vcLjSpY7GsMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3091dfea-51ab-43ac-e1bc-49eed0e5f26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ответ:\n",
            "Выход: я не знаю\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Расскажи мне об интегралах\""
      ],
      "metadata": {
        "id": "xcZUpsaOyEaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_template =f\"\"\"<s>system\n",
        "        Отвечай в соответствии с Источником. Проверь, есть ли в Источнике упоминания о ключевых словах Вопроса.\n",
        "        Если нет, то просто скажи: 'я не знаю'. Не придумывай!</s>\n",
        "        <s>user\n",
        "        Вопрос: {query}\n",
        "        Источник:\n",
        "        </s>\n",
        "        \"\"\"\n",
        "\n",
        "response = query_engine.query(message_template)\n",
        "\n",
        "print()\n",
        "print('Ответ:')\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "zYSvw1e7yEaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77bd43e-d561-4018-a27a-0507cb614652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ответ:\n",
            "Вы хотите получить информацию о интегралах? Это сложная тема, которая требует специального изучения. Интегралы используются в математике для вычисления площадей под графиками функций, а также для решения многих других задач. Если вам нужно получить более подробную информацию, я могу предоставить вам ссылки на учебные материалы или даже помочь найти специалиста в этой области.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тесты"
      ],
      "metadata": {
        "id": "sEgYirm0aHkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Как приготовить суп?\",\n",
        "    \"Как подключить услугу информационно-справочного обслуживания через электронные средства связи\",\n",
        "    \"Как подключить услугу коротких номеров?\",\n",
        "    \"Какова стоимость оказания услуги коротких номеров?\",\n",
        "    \"Что за компания мегафон?\",\n",
        "    \"Что за компания Газпром?\",\n",
        "    \"Что за компания Yota?\",\n",
        "    \"Что за компания Билайн?\",\n",
        "    \"Расскажи об условиях оказания услуг связи\"\n",
        "]"
      ],
      "metadata": {
        "id": "79s3giAgaI0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in questions:\n",
        "  message_template =f\"\"\"<s>system\n",
        "        Отвечай в соответствии с Источником. Проверь, есть ли в Источнике упоминания о ключевых словах Вопроса.\n",
        "        Если нет, то просто скажи: 'я не знаю'. Не придумывай!</s>\n",
        "        <s>user\n",
        "        Вопрос: {q}\n",
        "        Источник:\n",
        "        </s>\n",
        "        \"\"\"\n",
        "\n",
        "  response = query_engine.query(message_template)\n",
        "\n",
        "  print()\n",
        "  print('Ответ:')\n",
        "  print(response.response)"
      ],
      "metadata": {
        "id": "1sp8kzzebUqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3daf521-7983-434f-f500-47685e998273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ответ:\n",
            "Выход: я не знаю\n",
            "\n",
            "Ответ:\n",
            "Вы можете обратиться в службу поддержки оператора, они помогут вам подключить услугу информационно-спраwoчного обслуживания через электронные средства связи.\n",
            "\n",
            "Ответ:\n",
            "Отвечай в соответствии с Источником.\n",
            "Проверь, есть ли в Источнике упоминаний о ключевых словах Вопроса.\n",
            "Если нет, то просто скажи: 'я не znaju'. Не придумывай!\n",
            "\n",
            "Ответ:\n",
            "Стоимость Услуги оказывается единовременно по действующим tarifam, приведенным на сайте Оператора. Стоимость Услуги устана- валится Оператором в зависимости от типа заказываемого Мобильного контента и ука- дается на официальном сайте Оператора www.megafon.ru в разделе «Услуgi с использованием коротких номеров». Информация о стоимости Услуги предоставляется в места- \n",
            "х продаж и обслуживания Абонентов или по единому номеру справочно- информационной службы Оператора 0500.\n",
            "\n",
            "Ответ:\n",
            "МегаФон – это российский мобильный оператор, который предоставляет услуги связи и интернета для физических и юридических лиц. Он является одним из крупнейших мобильных операторов в России и предлагает широкий спектр планов и услуг для своих пользователей.\n",
            "\n",
            "Ответ:\n",
            "Газпром — российская международная газовая компания, одна из крупнейших в мире. Компания занимается добычей, транспортировкой, хранением, обработкой и реализацией газа. Газпром является одним из крупнейших производителей и экспортеров газа в мире. Компания активно развивает свою деятельность на мировом рынке, строит новые газовые трубопроводы и инфраструктуру для их обслуживания.\n",
            "\n",
            "Ответ:\n",
            "Я не знаю.\n",
            "\n",
            "Ответ:\n",
            "Билайн – российская мобильная телекоммуникационная компания, которая является одним из крупнейших операторов мобильной связи в России. Компания была основана в 2001 году и с тех пор активно развивается, предлагая свои услуги по всей стране. Билайн предоставляет различные виды мобильной связи, включая звонок, текстовое сообщение и интернет, а также предоставляет услуги по продаже мобильных телефонов и других устройств. Компания активно использует современные технологии и методы маркетинга, чтобы обеспечить высокий уровень качества своей работы и удовлетворить потребности своих клиентов.\n",
            "\n",
            "Ответ:\n",
            "Условия оказания услуг связи предусмотрены в договоре, подписанном между абонентом и оператором. В договоре устанавливаются такие условия, как:\n",
            "\n",
            "1. Срок действия договора - это может быть от одного года до нескольких лет, в зависимости от выбранного пакета услуг.\n",
            "2. Объем включенного трафика - это количество данных, которое может быть передано за определенный период времени.\n",
            "3. Цены за дополнительные услуги - это стоимость дополнительных функций, которые можно добавить к основному пакету, например, доступ к Wi-Fi или дополнительный объем трафика.\n",
            "4. Право на отказ от услуг - это право оператора отказаться от предоставления услуг, если абонент нарушил правила пользования, указанные в договоре.\n",
            "5. Право на изменение условий договора - это право оператора изменять условия договора, если это необходимо для оптимизации работы системы или изменения политики компании.\n",
            "6. Право на отзыв услуг - это право оператора отказаться от предоставить услуги, если абонент не оплатил за них.\n",
            "7. Право на обработку персональных данных - это право оператора обрабатывать персональные данные абонента в соответствии с законами и политикой компании.\n",
            "8. Право на предоставление информации о нарушениях - это право оператора предоставлять информацию о нарушениях правил пользования услугами связи.\n",
            "9. Право на предоставление информации о статистике использования услуг - это право оператора предоставлять информацию о статистике использования услуг связи.\n",
            "10. Право на предоставление информации о новых услугах - это право оператора предоставлять информацию об новых услугах связи.\n",
            "11. Право на предоставление информации о сроках и условиях предоставления услуг - это право оператора предоставлять инformation об сроках и условиях предоставления услуги связи.\n",
            "12. Право на предоставление информации о цене за услуги - это право оператора предоставлять информацию о цене за услуги связи.\n",
            "13. Право на предоставление информации о способе оплаты - это право оператора предоставлять информацию о способе оплаты за услуги связи.\n",
            "14. Право на предоставление информации о правилах пользования услугами связи - это право оператора предоставлять информацию о правилах пользования услугами связи.\n",
            "15. Право на предоставление информации о правилами обращения - это право оператора предоставлять информацию о практике обращения к службе поддержки.\n",
            "16. Право на предоставление информации о правилам предоставления услуг связи - это право оператора предоставлять информации о правилах предоставления услуг связи.\n",
            "17. Право на предоставление информации о правила и порядке прекращения предоставления услуг связи - это право оператора информировать абонента о правилах и порядке прекращения предоставления услуги связи.\n",
            "18. Право на предоставление информации о правила предоставления услуг связи - это право оператора проинформировать абонента о правилах предоставления услуг связи.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На тестировании видно, что модель время от времени галлюцинирует.\n",
        "\n",
        "RAG система работает не корректно, что может портебовать создание более сложной системы хранения данных."
      ],
      "metadata": {
        "id": "etJEpQ39Q6fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание интерфейса и инференс модели"
      ],
      "metadata": {
        "id": "3G1dJFvBgpjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Отприсовка интерфейса с помощью grad\n",
        "import gradio as gr\n",
        "# Gradio позволяет объединять элементы в блоки\n",
        "blocks = gr.Blocks()\n",
        "\n",
        "# Работаем с блоком\n",
        "with blocks as demo:\n",
        "    # Поле пользовательского запроса к LLM (по умолчанию поле query из models)\n",
        "    query = gr.Textbox(label=\"Запрос к LLM\", interactive=True)\n",
        "    request_btn = gr.Button(\"Отправить запрос\")    # кнопка отправки запроса к LLM\n",
        "\n",
        "    def load_index(docs):\n",
        "        vector_index = load_index_from_storage(storage_context)\n",
        "        query_engine = vector_index.as_query_engine()\n",
        "\n",
        "    # Функция для запроса к LLM\n",
        "    def predict(q):\n",
        "        message_template = f\"\"\"<s>system\n",
        "        Отвечай в соответствии с Источником. Проверь, есть ли в Источнике упоминания о ключевых словах Вопроса.\n",
        "        Если нет, то просто скажи: 'я не знаю'. Не придумывай! </s>\n",
        "        <s>user\n",
        "        Вопрос: {q}\n",
        "        Источник:\n",
        "        </s>\n",
        "        \"\"\"\n",
        "\n",
        "        response = query_engine.query(message_template)\n",
        "        # возвращает список из ответа от LLM\n",
        "        return response.response\n",
        "\n",
        "\n",
        "\n",
        "    # Выводим поля response с ответом от LLM\n",
        "    with gr.Row():\n",
        "        response = gr.Textbox(label=\"Ответ LLM\") # Текстовое поле с ответом от LLM\n",
        "\n",
        "    # При нажатии на кнопку request_btn запускается функция отправки запроса к LLM request_btn с параметром query\n",
        "    # Результат выполнения функции сохраняем в текстовые поля  response - ответ модели\n",
        "    request_btn.click(predict, query, response)\n",
        "\n",
        "# Запуск приложения\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "au-cCpXz5p2Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "4d0f66e7-4ccb-4c48-919f-34785528953f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1002: UserWarning: Expected 1 arguments for function <function load_index at 0x7f747c7cf0a0>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1006: UserWarning: Expected at least 1 arguments for function <function load_index at 0x7f747c7cf0a0>, received 0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://024ca54097fe8d763d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://024ca54097fe8d763d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}
